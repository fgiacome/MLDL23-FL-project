{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyME4mkpH7o/1Nf/adi4Sk1R"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**Packages, Libraries and others**"],"metadata":{"id":"3-t1MPPVryl7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3-Msh7kLruCP"},"outputs":[],"source":["# PyTorch\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Torchvision\n","import torchvision\n","from torchvision import transforms as T\n","from torchvision import datasets\n","\n","# Import Numpy\n","import numpy as np\n","\n","# Import Pyplot\n","import matplotlib.pyplot as plt\n","\n","# Import train_test_split\n","from sklearn.model_selection import train_test_split\n","\n","# Set device\n","import copy\n","import random\n","from collections import OrderedDict\n","from math import pi"]},{"cell_type":"markdown","source":["**Client and Servers implementations**"],"metadata":{"id":"KahQs8L0r9mk"}},{"cell_type":"code","source":["# CLIENT\n","class Client:\n","\n","  def __init__(self, client_id, dataset, batch_size, model):\n","    self.client_id = client_id\n","    self.dataset = dataset\n","    self.model = model\n","    self.criterion = nn.CrossEntropyLoss()\n","    self.dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n","    self.num_samples = len(self.dataloader) * batch_size\n","\n","  def run_epoch(self, optimizer):\n","    cumulative_loss = 0\n","    for images, labels in self.dataloader:\n","      optimizer.zero_grad()\n","      labels_hat = self.model.forward(images)\n","      loss = self.criterion(labels_hat, labels)\n","      loss.backward()\n","      optimizer.step()\n","      cumulative_loss += loss.item()\n","    return cumulative_loss / self.num_samples\n","\n","  def train(self, num_epochs):\n","    optimizer = torch.optim.Adam(self.model.parameters(),\n","                                 lr = 1e-3, weight_decay = 1e-4)\n","    self.model.train()\n","    loss_track = torch.empty(num_epochs)\n","    for epoch in range(num_epochs):\n","      loss_track[epoch] = self.run_epoch(optimizer)\n","    return loss_track\n","\n","  def test(self):\n","    cumulative_loss = 0\n","    cumulative_accuracy = 0\n","    num_samples = 0\n","    self.model.eval()\n","    with torch.no_grad():\n","      for images, labels in self.dataloader:\n","        labels_hat = self.model.forward(images)\n","        labels_pred = labels_hat.argmax(dim = 1)\n","        cumulative_loss += self.criterion(labels_hat, labels).item()\n","        cumulative_accuracy += labels_pred.eq(labels).sum().item()\n","        num_samples += images.shape[0]\n","    return cumulative_loss / num_samples, cumulative_accuracy / num_samples\n","\n","  def generate_update(self):\n","    return copy.deepcopy(self.model.state_dict())"],"metadata":{"id":"1TNOfTQKryOX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SERVER AVG\n","class Server_fedavg:\n","\n","  def __init__(self, train_clients, test_clients, model,\n","               num_rounds, epochs_per_round, num_clients_per_round, random_state = 1):\n","    self.train_clients = train_clients\n","    self.test_clients = test_clients\n","    self.model = model\n","    self.model_params_dict = self.model.state_dict()\n","    self.num_rounds = num_rounds\n","    self.epochs_per_round = epochs_per_round\n","    self.num_clients_per_round = num_clients_per_round\n","    self.weights = {client.client_id: 1 / len(self.train_clients) \\\n","                    for client in self.train_clients}\n","    self.updates = []\n","    self.prng = np.random.default_rng(random_state)\n","\n","  def load_model_on_clients(self):\n","    for client in self.train_clients + self.test_clients:\n","      client.model.load_state_dict(self.model_params_dict, strict = False)\n","\n","  def select_clients(self):\n","    return self.prng.choice(\n","        self.train_clients,\n","        size = self.num_clients_per_round,\n","        replace = False,\n","        )\n","\n","  def train_round(self, train_clients):\n","    client_loss = dict()\n","    for client in train_clients:\n","      client_loss[client.client_id] = client.train(self.epochs_per_round)\n","      self.updates.append((client.client_id, client.generate_update()))\n","    return client_loss\n","\n","  def update_model(self):\n","    base = OrderedDict()\n","    for client_id, client_model in self.updates:\n","      for key, value in client_model.items():\n","        if key in base:\n","          base[key] += value.type(torch.FloatTensor) / self.num_clients_per_round\n","        else:\n","          base[key] = value.type(torch.FloatTensor) / self.num_clients_per_round\n","      for key, value in base.items():\n","        self.model_params_dict[key] = value\n","    self.model.load_state_dict(self.model_params_dict, strict = False)\n","    self.load_model_on_clients()\n","\n","  def train(self):\n","    results = []\n","    for round in range(self.num_rounds):\n","      train_clients = self.select_clients()\n","      client_loss = self.train_round(train_clients)\n","      results.append(client_loss)\n","      self.update_model()\n","      self.updates = []\n","    return results\n","\n","  def train_evaluation(self):\n","    results = {\n","        'Train': [],\n","        'Test': []\n","    }\n","    for round in range(self.num_rounds):\n","      print(f'Round {round + 1}')\n","      train_clients = self.select_clients()\n","      client_loss = self.train_round(train_clients)\n","      self.update_model()\n","      self.updates = []\n","      # Compute mean accuracy on train set\n","      acc = 0\n","      stats = self.eval_train()\n","      for client, res in stats.items():\n","        acc += res['Accuracy'] / len(self.train_clients)\n","      results['Train'].append(acc)\n","      # Compute mean accuracy on test set\n","      acc = 0\n","      stats = self.test()\n","      for client, res in stats.items():\n","        acc += res['Accuracy'] / len(self.test_clients)\n","      results['Test'].append(acc)\n","    return results\n","\n","  def eval_train(self):\n","      self.load_model_on_clients()\n","      eval_statistics = {c.client_id: {} for c in self.train_clients}\n","      for c in self.train_clients:\n","          l, m = c.test()\n","          eval_statistics[c.client_id][\"Loss\"] = l\n","          eval_statistics[c.client_id][\"Accuracy\"] = m\n","      return eval_statistics\n","\n","  def test(self):\n","      self.load_model_on_clients()\n","      eval_statistics = {c.client_id: {} for c in self.test_clients}\n","      for c in self.test_clients:\n","          l, m = c.test()\n","          eval_statistics[c.client_id][\"Loss\"] = l\n","          eval_statistics[c.client_id][\"Accuracy\"] = m\n","      return eval_statistics"],"metadata":{"id":"gUV3GSXLJVR_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SERVER GW\n","class Server:\n","    def __init__(\n","        self,\n","        num_clients_per_round,\n","        num_rounds,\n","        epochs_per_round,\n","        train_clients: list[Client],\n","        test_clients: list[Client],\n","        model: torch.nn.Module,\n","        use_prior,\n","        n_rounds_no_prior,\n","        random_state=300890,\n","    ):\n","        self.clients_per_round = num_clients_per_round\n","        self.num_rounds = num_rounds\n","        self.train_clients = train_clients\n","        self.test_clients = test_clients\n","        self.model = model\n","        self.epochs_per_round = epochs_per_round\n","        self.use_prior = use_prior\n","        self.n_rounds_no_prior = int(n_rounds_no_prior * num_rounds)\n","        self.weights = OrderedDict(\n","            (client.client_id, 1 / len(self.train_clients))\n","            for client in self.train_clients\n","        )\n","        self.weights_track = [self.weights.copy()]\n","        self.epochs_stds = None\n","        self.model_params_dict = copy.deepcopy(self.model.state_dict())\n","        self.updates = []\n","        self.prng = np.random.default_rng(random_state)\n","\n","    def select_clients(self):\n","        \"\"\"\n","        This method selects a random subset of `self.clients_per_round` clients\n","        from the given traning clients, without replacement.\n","        :return: list of clients\n","        \"\"\"\n","        if self.use_prior == True:\n","            return self.prng.choice(\n","                self.train_clients,\n","                size=self.clients_per_round,\n","                replace=False,\n","                p=[w for _, w in self.weights.items()],\n","            )\n","        else:\n","            return self.prng.choice(\n","                self.train_clients, size=self.clients_per_round, replace=False\n","            )\n","\n","    def load_model_on_clients(self):\n","        \"\"\"\n","        This function loads the centralized model to the clients at\n","        the beginning of each training / testing round.\n","        \"\"\"\n","        for c in self.test_clients + self.train_clients:\n","            c.model.load_state_dict(self.model_params_dict, strict=False)\n","\n","    def train_round(self, clients: list[Client]):\n","        \"\"\"\n","        This method trains the model with the dataset of the clients.\n","        It handles the training at single round level.\n","        The client updates are saved in the object-level list,\n","        they will be aggregated.\n","        :param clients: list of all the clients to train\n","        \"\"\"\n","        client_loss = OrderedDict()\n","        for client in clients:\n","            client_loss[client.client_id] = client.train(self.epochs_per_round)\n","            self.updates.append((client.client_id, client.generate_update()))\n","        return client_loss\n","\n","    def update_weights(self, client_loss: OrderedDict):\n","        \"\"\"\n","        Updates the weigths saved at instance level.\n","        :param client_loss: a dictionary client_id -> list of round losses\n","        :return: the sum of the weights of the selected clients\n","        \"\"\"\n","        # Normalize selected clients' weights\n","        w_sum = 0\n","        for client_id in client_loss.keys():\n","            w_sum += self.weights[client_id]\n","        for client_id in client_loss.keys():\n","            self.weights[client_id] = self.weights[client_id] / w_sum\n","\n","        # Compute the mean process\n","        weights_sum = 0\n","        weights2_sum = 0\n","        loss_tensor = torch.empty(len(client_loss), self.epochs_per_round)\n","        mean_loss = torch.zeros(self.epochs_per_round)\n","        for idx, client_id in enumerate(client_loss.keys()):\n","            loss_tensor[idx] = client_loss[client_id]\n","            mean_loss += self.weights[client_id] * client_loss[client_id]\n","            weights_sum += self.weights[client_id] / len(client_loss)\n","            weights2_sum += (self.weights[client_id] ** 2) / len(client_loss)\n","\n","        # Compute the standard deviation for each epoch\n","        sigma2 = ((loss_tensor - mean_loss) ** 2).sum(dim=0) / (len(client_loss) - 1)\n","        std_loss = sigma2 * (weights2_sum / weights_sum ** 2) * (1 / len(client_loss))\n","\n","        self.epochs_stds = torch.sqrt(std_loss)\n","\n","        # Compute the rewards for each epoch and each client\n","        exp_args_tensor = 0.5 * ((loss_tensor - mean_loss) ** 2) / std_loss\n","        reward_tensor = torch.exp(-exp_args_tensor).mean(dim=1)\n","        reward_tensor = reward_tensor / reward_tensor.sum()\n","\n","        # Assign rewards to weights\n","        for idx, client_id in enumerate(client_loss.keys()):\n","            self.weights[client_id] = reward_tensor[idx].item() * w_sum\n","        return w_sum\n","\n","    def aggregate(self, inv_scale_factor):\n","        \"\"\"\n","        This method handles the FedAvg aggregation\n","        :param inv_scale_factor: scales the weights by the inverse of this factor\n","        :return: aggregated parameters\n","        \"\"\"\n","        # Here we make the average of the updated weights\n","        base = OrderedDict()\n","        for client_id, client_model in self.updates:\n","            for key, value in client_model.items():\n","                if key in base:\n","                    base[key] += (\n","                        (1 / inv_scale_factor)\n","                        * self.weights[client_id]\n","                        * value.type(torch.FloatTensor)\n","                    )\n","                else:\n","                    base[key] = (\n","                        (1 / inv_scale_factor)\n","                        * self.weights[client_id]\n","                        * value.type(torch.FloatTensor)\n","                    )\n","        for key, value in base.items():\n","            self.model_params_dict[key] = value.to(\"cuda\")\n","\n","        self.model.load_state_dict(self.model_params_dict, strict=False)\n","        self.updates = []\n","\n","    def train(self, path=None):\n","        \"\"\"\n","        This method orchestrates the training the evals and tests at rounds level\n","        :return: Train / test statistics at each round. \"Train as it happens\" is the typical epoch loss returned from each client.\n","        \"\"\"\n","        orchestra_statistics = {\"Test\": [], \"Train as it happens\": []}\n","\n","        for r in range(self.num_rounds):\n","            self.load_model_on_clients()\n","            if (r >= self.n_rounds_no_prior) and (self.use_prior == False):\n","                self.use_prior = True\n","            print(f\"Round {r + 1}\")\n","            clients = self.select_clients()\n","            clients_loss = self.train_round(clients)\n","            constant_w_sum = self.update_weights(clients_loss)\n","            for key in clients_loss.keys():\n","                clients_loss[key] = clients_loss[key].cpu().tolist()\n","            orchestra_statistics[\"Train as it happens\"].append(clients_loss)\n","            self.aggregate(constant_w_sum)\n","\n","            # normalize all weights (this normalization pass is extra)\n","            weights_sum = 0\n","            for _, w in self.weights.items():\n","                weights_sum += w\n","            for client_id, w in self.weights.items():\n","                self.weights[client_id] = w / weights_sum\n","            self.weights_track.append(self.weights.copy())\n","\n","            # # compute mean accuracy on train set\n","            # acc = 0\n","            # stats = self.eval_train()\n","            # for _, res in stats.items():\n","            #     acc += res[\"mIoU\"][\"Mean IoU\"] / len(self.train_clients)\n","            # orchestra_statistics[\"Train\"].append(acc)\n","\n","            # compute mean accuracy on test set\n","            stats = self.test()\n","            orchestra_statistics[\"Test\"].append(stats)\n","\n","            if path is not None:\n","                self.save_checkpoint(path + f\"_{r}.json\", r)\n","        return orchestra_statistics\n","\n","    def eval_train(self):\n","        \"\"\"\n","        This method handles the evaluation on the train clients\n","        :return: dict (one key per client) of dicts (loss, miou) of scalars\n","        \"\"\"\n","        self.load_model_on_clients()\n","        eval_statistics = {c.client_id: {} for c in self.train_clients}\n","        for c in self.train_clients:\n","            l, m = c.test()\n","            eval_statistics[c.client_id][\"Loss\"] = l\n","            eval_statistics[c.client_id][\"Accuracy\"] = m\n","        return eval_statistics\n","\n","    def test(self):\n","        \"\"\"\n","        This method handles the test on the test clients\n","        :return: dict (one key per client) of dicts (loss, miou) of scalars\n","        \"\"\"\n","        self.load_model_on_clients()\n","        eval_statistics = {c.client_id: {} for c in self.test_clients}\n","        for c in self.test_clients:\n","            l, m = c.test()\n","            eval_statistics[c.client_id][\"Loss\"] = l\n","            eval_statistics[c.client_id][\"Accuracy\"] = m\n","        return eval_statistics\n","\n","    def save_checkpoint(self, path, round):\n","        torch.save(\n","            {\n","                \"round\": round,\n","                \"model_state_dict\": self.model.state_dict()\n","            }, path\n","        )"],"metadata":{"id":"GFdZ-RxHJVZf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**LeNet-5**"],"metadata":{"id":"vjOvzJDdtkS8"}},{"cell_type":"code","source":["class LeNet5(nn.Module):\n","\n","    def __init__(self, num_classes):\n","        super(LeNet5, self).__init__()\n","\n","        self.feature_extractor = nn.Sequential(\n","            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1,\n","                      padding = 2),\n","            nn.Tanh(),\n","            nn.AvgPool2d(kernel_size=2),\n","            nn.BatchNorm2d(6),\n","            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),\n","            nn.Tanh(),\n","            nn.AvgPool2d(kernel_size=2),\n","            nn.BatchNorm2d(16)\n","        )\n","\n","        self.classifier = nn.Sequential(\n","            nn.Linear(in_features = 400, out_features = 120),\n","            nn.Tanh(),\n","            nn.Linear(in_features=120, out_features=84),\n","            nn.Tanh(),\n","            nn.Linear(in_features=84, out_features=num_classes),\n","        )\n","\n","\n","    def forward(self, x):\n","        x = self.feature_extractor(x)\n","        x = torch.flatten(x, 1)\n","        logits = self.classifier(x)\n","        # return F.softmax(logits, dim=1)\n","        return logits\n","\n","model = LeNet5(num_classes = 10).cuda()"],"metadata":{"id":"yVhA0Tk1te7H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Fashion MNIST**"],"metadata":{"id":"5W2b0AKctZw8"}},{"cell_type":"code","source":["# Images standardizer\n","def images_scaler(X):\n","  X_std = torch.empty(len(X), 28, 28)\n","  y = torch.empty(len(X))\n","  for idx, sample in enumerate(X):\n","    x, label = sample\n","    mu = x.mean()\n","    std = torch.sqrt(torch.sum((x - mu) ** 2) / (28 ** 2 - 1))\n","    X_std[idx] = (x[0] - mu) / std\n","    y[idx] = label\n","  return X_std, y"],"metadata":{"id":"6fd5HSlYryRZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load MNIST dataset\n","mnist_train = datasets.FashionMNIST('data', train=True, download=True, transform=T.ToTensor())\n","mnist_test = datasets.FashionMNIST('data', train=False, download=True, transform=T.ToTensor())\n","\n","# Standardizzazione\n","X_dev, y_dev = images_scaler(mnist_train)\n","X_dev = X_dev.view(len(X_dev), 1, 28, 28)\n","X_eval, y_eval = images_scaler(mnist_test)\n","X_eval = X_eval.view(len(X_eval), 1, 28, 28)"],"metadata":{"id":"VkLFbpk1te-n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate clients datasets\n","dataset = [(X_dev[i].cuda(), y_dev[i].type(torch.LongTensor).cuda()) for i in range(len(X_dev))]\n","dataset_n1 = [((X_dev[i] + 1 * torch.rand(1, 28, 28)).cuda(), y_dev[i].type(torch.LongTensor).cuda()) for i in range(len(X_dev))]\n","dataset_n2 = [((X_dev[i] + 2 * torch.rand(1, 28, 28)).cuda(), y_dev[i].type(torch.LongTensor).cuda()) for i in range(len(X_dev))]\n","dataset_fl = dataset_fl = [(X_dev[i].cuda(), y_dev[i].type(torch.LongTensor).cuda()) for i in range(len(X_dev)) if y_dev[i] == 2]\n","train_clients_df = [train_test_split(dataset, y_dev, train_size = .3, shuffle = True, random_state = i)[0] \\\n","    for i in range(18)] + [\n","    train_test_split(dataset_n1, y_dev, train_size = .3, shuffle = True, random_state = 100)[0],\n","    train_test_split(dataset_n1, y_dev, train_size = .3, shuffle = True, random_state = 101)[0],\n","    train_test_split(dataset_n2, y_dev, train_size = .3, shuffle = True, random_state = 102)[0],\n","    train_test_split(dataset_n2, y_dev, train_size = .3, shuffle = True, random_state = 103)[0],\n","    train_test_split(dataset_n2, y_dev, train_size = .3, shuffle = True, random_state = 104)[0],\n","    train_test_split(dataset_fl, y_dev[y_dev == 2], train_size = .3, shuffle = True, random_state = 5)[0]\n","]\n","\n","test_clients_df = [\n","    train_test_split(dataset, y_dev, train_size = .3, shuffle = True, random_state = 301)[0],\n","    train_test_split(dataset, y_dev, train_size = .3, shuffle = True, random_state = 302)[0],\n","    train_test_split(dataset, y_dev, train_size = .3, shuffle = True, random_state = 303)[0],\n","]\n","\n","# Initialize clients\n","train_clients = [\n","    Client(client_id = f'c_{i}', dataset = train_clients_df[i],\n","           batch_size = 128, model = deepcopy(model)) for i in range(len(train_clients_df))\n","]\n","\n","# Test clients\n","test_clients = [\n","    Client(client_id = f'c_test_{i}', dataset = test_clients_df[i],\n","           batch_size = 256, model = deepcopy(model)) for i in range(3)\n","]"],"metadata":{"id":"G9YU8WuotfB4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Number of rounds\n","num_rounds = 20\n","\n","# Choose Server (FedGW) or Server_fedavg (FedAvg)\n","strategy = 'FedGW'\n","\n","# Training\n","if strategy == 'FedAvg':\n","  server = Server_fedavg(num_rounds = num_rounds, num_clients_per_round = len(train_clients),\n","                  epochs_per_round = 6,\n","                  train_clients = train_clients, test_clients = test_clients,\n","                  model = copy.deepcopy(model))\n","  results = server.train_evaluation()\n","elif strategy == 'FedGW':\n","  server = Server(num_rounds = num_rounds, num_clients_per_round = len(train_clients),\n","                  epochs_per_round = 6, use_prior = False, n_rounds_no_prior = .6,\n","                  train_clients = train_clients, test_clients = test_clients,\n","                  model = copy.deepcopy(model))\n","  results = server.train()"],"metadata":{"id":"STlAX_LQvn9a"},"execution_count":null,"outputs":[]}]}