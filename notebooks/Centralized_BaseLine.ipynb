{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"mFEp1V5RlrcK"},"source":["**IMPORTANTE:** \n","- Il runtime di default Ã¨ **CPU** per non consumare le ore sulle **GPU**, ricordarsi di cambiarlo;\n","\n","**Collegamento a myDrive per download del dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20332,"status":"ok","timestamp":1684395454552,"user":{"displayName":"Davide Leo","userId":"07819918969107647540"},"user_tz":-120},"id":"8PN5CgCglhkR","outputId":"38a18005-48dc-4017-ed7a-d107c75a71e2"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"NaXuHszwnMGj"},"source":["**Clone del repository github**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1684395454554,"user":{"displayName":"Davide Leo","userId":"07819918969107647540"},"user_tz":-120},"id":"O8evumfrllb6","outputId":"db7a9df8-f2df-4d4f-ecf3-50900c494627"},"outputs":[],"source":["!git clone https://github.com/fgiacome/MLDL23-FL-project.git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":290,"status":"ok","timestamp":1684395454833,"user":{"displayName":"Davide Leo","userId":"07819918969107647540"},"user_tz":-120},"id":"kKNk_DTUlliz","outputId":"efc2eb41-d36f-411a-e789-82021c0ca500"},"outputs":[],"source":["import sys\n","sys.path.append('./MLDL23-FL-project/')\n","%cd MLDL23-FL-project"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"QTe5YKGlvZjH"},"source":["**Importazione del dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eNRbBNGhllpg"},"outputs":[],"source":["# Funzioni utili\n","import main\n","from models import deeplabv3, mobilenetv2\n","\n","# Get dataset (use static variables for args class)\n","class args:\n","  dataset = 'idda'\n","  model = 'deeplabv3_mobilenetv2'\n","\n","df = main.get_datasets(args())"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"IkR3c99mHqdN"},"source":["**Verifica del funzionamento del dataset**\n","\n","Se si vuole verificare il che sia tutto ok prima della fase di training settate **check == True**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oHU6rVJvv-5s"},"outputs":[],"source":["# Check variable\n","check = False\n","\n","# Verifica implementazione __getitem__\n","# Importazione Numpy e Pyplot \n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import random\n","import json, os\n","from datasets.idda import IDDADataset\n","from datasets import ss_transforms as sstr\n","\n","if check:\n","\n","  # The chosen ones\n","  # constrast 0.6 1.6\n","  # hue -0.05 .05\n","  # brightness 0.55 1.6\n","  # saturation 0.5 1.6\n","\n","  # Transforms\n","  train_transforms = sstr.Compose([\n","              sstr.ColorJitter(brightness = (0.55, 1.6), contrast = (0.6, 1.6), \n","                                saturation = (0.5, 1.6), hue = (-0.05, 0.05)),\n","              sstr.RandomRotation(degrees = (-5, 5)),\n","              sstr.PadCenterCrop((925, 1644), fill=255), # 9:16 --> togliere i bordi\n","              sstr.Resize((512, 928)), # pixel utili per non far impazzire GPU\n","              sstr.ToTensor(),\n","              sstr.Normalize(mean=[0.485, 0.456, 0.406], \n","                              std=[0.229, 0.224, 0.225])\n","          ])\n","  val_transforms = sstr.Compose([sstr.Resize((512, 928)),\n","                                 sstr.ToTensor(),\n","                                  sstr.Normalize(mean=[0.485, 0.456, 0.406], \n","                                                std=[0.229, 0.224, 0.225])])\n","\n","  # Import datasets\n","  # Set random seed (RICORDATEVELO)\n","  random.seed(300890)\n","\n","  # codice copiato da get_datasets\n","  root = 'data/idda'\n","  client_id = \"CENTRALIZED\"\n","  with open(os.path.join(root, 'train.json'), 'r') as f:\n","      all_data = json.load(f)\n","  train_img_names = [] # Lista di stringhe da shuffle con metodo nativo random\n","  for k in all_data.keys():\n","      for i in all_data[k]:\n","        train_img_names.append(i)\n","\n","  # Shuffle data and train-validation splitting\n","  # random.shuffle(train_img_names)\n","\n","  # Split in train/validation\n","  train_size = int(600 * .7)\n","  val_img_names = train_img_names[train_size:].copy()\n","  train_img_names = train_img_names[:train_size].copy()\n","\n","  # Generate train-validation sets\n","  train_dataset = IDDADataset(root = root, list_samples = train_img_names, \n","                              transform = train_transforms, client_name = client_id)\n","\n","  val_dataset = IDDADataset(root = root, list_samples = val_img_names, \n","                            transform = val_transforms, client_name = client_id)\n","\n","  # Estrazione item 0 del client 0\n","  img, target = train_dataset.__getitem__(3)\n","  img_np = img.numpy()\n","  target_np = target.numpy()\n","\n","  # Il modulo main normalizza l'immagine\n","  # Per visualizzarla, la denormalizziamo\n","  def denormalize(img):\n","    img[0,:] *= 0.229\n","    img[1,:] *= 0.224\n","    img[2,:] *= 0.225\n","    img[0,:] += 0.485\n","    img[1,:] += 0.456\n","    img[2,:] += 0.460\n","    return img\n","\n","  # Reshaping per pyplot\n","  # Pyplot richiede np.array di shape (base, altezza, n_channels)\n","  def plt_reshape(img):\n","    img = np.transpose(img, (1, 2, 0))\n","    return img\n","\n","  # Plot\n","  fig, axs = plt.subplots(nrows = 1, ncols = 2, figsize = (15, 5))\n","  axs[0].set_title('Image')\n","  axs[0].imshow(plt_reshape(denormalize(img)))\n","  axs[0].set_xticks([])\n","  axs[0].set_yticks([])\n","  axs[0].set_xticklabels([])\n","  axs[0].set_yticklabels([])\n","  axs[1].set_title('Target')\n","  axs[1].imshow(target_np)\n","  axs[1].set_xticks([])\n","  axs[1].set_yticks([])\n","  axs[1].set_xticklabels([])\n","  axs[1].set_yticklabels([])"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"hHFbOs70JDLc"},"source":["**Training and test function**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sgQkQMdHzMPm"},"outputs":[],"source":["# Useful function for model training and testing\n","# Import Intersect Over Union\n","from utils.stream_metrics import StreamSegMetrics\n","\n","# Training function\n","def train(net, data_loader, loss_function, optimizer):\n","\n","  # Loop initialization\n","  samples = 0\n","  cumulative_loss = 0\n","  mean_iou = StreamSegMetrics(n_classes = 16, name = 'Mean IoU')\n","\n","  # Set the training mode to the model\n","  net.train()\n","\n","  # Loop over batches\n","  for idx, (X, y) in enumerate(data_loader):\n","\n","    # Send data to GPU\n","    X = X.cuda()\n","    # y = y.long()\n","    y = y.cuda()\n","\n","    # Predictions\n","    y_hat = net(X)['out']\n","    y_pred = torch.argmax(y_hat, dim = 1)\n","\n","    # Compute loss\n","    loss = loss_function(y_hat, y)\n","\n","    # Backpropagation\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","\n","    # Update training variables\n","    samples += X.size(0)\n","    cumulative_loss += loss.item() \n","    mean_iou.update(y.cpu().numpy(), y_pred.cpu().numpy())\n","\n","  return cumulative_loss / samples, mean_iou.get_results()\n","\n","\n","\n","# Test function\n","def test(net, data_loader, loss_function):\n","\n","  # Test variables initialization\n","  samples = 0\n","  cumulative_loss = 0\n","  mean_iou = StreamSegMetrics(n_classes = 16, name = 'Mean IoU')\n","\n","  # Set the evaluation mode to the model\n","  net.eval()\n","\n","  # Test loop (we don't want to compute the gradients)\n","  with torch.no_grad():\n","\n","    # Loop over batches\n","    for idx, (X, y) in enumerate(data_loader):\n","\n","      # Send data to the GPU\n","      X = X.cuda()\n","      # y = y.long()\n","      y = y.cuda()\n","\n","      # Compute predictions\n","      y_hat = net(X)['out']\n","      y_pred = torch.argmax(y_hat, dim = 1)\n","      \n","      # Loss computation\n","      loss = loss_function(y_hat, y)\n","\n","      # Update test variables\n","      samples += X.size(0)\n","      cumulative_loss += loss.item()\n","      mean_iou.update(y.cpu().numpy(), y_pred.cpu().numpy())\n","\n","  return cumulative_loss / samples, mean_iou.get_results()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qV5NgWC-wsPM"},"source":["**Iperparametri da testare**:\n","\n","In funzione dei parametri da testare aggiornare la variable **hyperparameters** a fondo cella."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SVY4sXGcplEW"},"outputs":[],"source":["###################################################################################\n","# Comb_list formats: [<Optimizer>, <learning_rate>, <batch_size>, <transforms>] #\n","###################################################################################\n","\n","# Alessandro\n","ale_hyper = {'comb1': ['Adam', 1e-2, 4, True],\n","             'comb2': ['Adam', 1e-2, 8, True],\n","             'comb3': ['Adam', 1e-1, 4, True],\n","             'comb4': ['Adam', 1e-1, 8, True],\n","             'comb5': ['best_optimizer', 'best_batch_size', False]} # Ultima riga da inserire (BEST COMB + NO TRANSFORMS)\n","\n","# Davide\n","dave_hyper = {'comb1': ['Adam', 1e-3, 4, True],\n","              'comb2': ['Adam', 1e-3, 8, True],\n","              'comb3': ['SGD', 1e-4, 4, True],\n","              'comb4': ['SGD', 1e-4, 8, True],\n","              'comb5': ['Adam', 1e-3, 8, False]} # Ultima riga da inserire (BEST COMB + NO TRANSFORMS)\n","\n","# Francesco\n","fra_hyper = {'comb1': ['SGD', 1e-3, 4, True],\n","             'comb2': ['SGD', 1e-3, 8, True],\n","             'comb3': ['SGD', 1e-2, 4, True],\n","             'comb4': ['SGD', 1e-2, 8, True],\n","             'comb5': ['best_optimizer', 'best_batch_size', False]} # Ultima riga da inserire (BEST COMB + NO TRANSFORMS)\n","\n","# Hyperparameters \n","hyperparameters = dave_hyper"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"z4wEO1Opws_p"},"source":["**Training/Test Loop**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1752901,"status":"ok","timestamp":1684397315406,"user":{"displayName":"Davide Leo","userId":"07819918969107647540"},"user_tz":-120},"id":"4KM_4L7XplIm","outputId":"dbeae2b2-e0a6-4e59-ad75-369bb33eb6c7"},"outputs":[],"source":["# Importations\n","import torch\n","import random\n","import json, os\n","from datasets.idda import IDDADataset\n","from datasets import ss_transforms as sstr\n","\n","# Model initialization\n","class args:\n","  dataset = 'idda'\n","  model = 'deeplabv3_mobilenetv2'\n","\n","model = main.model_init(args).cuda()\n","\n","################################################################################\n","############################## CHOOSE COMB! ####################################\n","################################################################################\n","comb = 'comb5' #################################################################\n","################################################################################\n","############################## CHOOSE COMB! ####################################\n","################################################################################\n","\n","# Transforms\n","##################################################################################\n","# Comb_list formats: [<Optimizer>, <learning_rate>, <batch_size>, <transforms>] #\n","#################### [     0     ,        1       ,       2     ,       3     ] #\n","##################################################################################\n","\n","if hyperparameters[comb][3] == True:\n","  # Transforms\n","  train_transforms = sstr.Compose([\n","              sstr.ColorJitter(brightness = (0.55, 1.6), contrast = (0.6, 1.6), \n","                                saturation = (0.5, 1.6), hue = (-0.05, 0.05)),\n","              sstr.RandomRotation(degrees = (-5, 5)),\n","              sstr.PadCenterCrop((925, 1644), fill=255), # 9:16 --> togliere i bordi\n","              sstr.Resize((512, 928)),\n","              sstr.ToTensor(),\n","              sstr.Normalize(mean=[0.485, 0.456, 0.406], \n","                              std=[0.229, 0.224, 0.225])\n","          ])\n","  val_transforms = sstr.Compose([sstr.Resize((512, 928)),\n","                                 sstr.ToTensor(),\n","                                  sstr.Normalize(mean=[0.485, 0.456, 0.406], \n","                                                std=[0.229, 0.224, 0.225])])\n","  test_transforms = sstr.Compose([sstr.Resize((512, 928)),\n","                                  sstr.ToTensor(),\n","                                  sstr.Normalize(mean=[0.485, 0.456, 0.406], \n","                                                std=[0.229, 0.224, 0.225])])\n","\n","else:\n","\n","  train_transforms = sstr.Compose([\n","              sstr.Resize((512, 928)),\n","              sstr.ToTensor(),\n","              sstr.Normalize(mean=[0.485, 0.456, 0.406], \n","                             std=[0.229, 0.224, 0.225])\n","          ])\n","  val_transforms = sstr.Compose([sstr.Resize((512, 928)),\n","                                 sstr.ToTensor(),\n","                                 sstr.Normalize(mean=[0.485, 0.456, 0.406], \n","                                                std=[0.229, 0.224, 0.225])])\n","  test_transforms = sstr.Compose([sstr.Resize((512, 928)),\n","                                  sstr.ToTensor(),\n","                                  sstr.Normalize(mean=[0.485, 0.456, 0.406], \n","                                                std=[0.229, 0.224, 0.225])])\n","  \n","# Set random seed\n","random.seed(300890)\n","\n","# codice copiato da get_datasets\n","root = 'data/idda'\n","client_id = \"CENTRALIZED\"\n","with open(os.path.join(root, 'train.json'), 'r') as f:\n","    all_data = json.load(f)\n","train_img_names = [] # Lista di stringhe da shuffle con metodo nativo random\n","for k in all_data.keys():\n","    for i in all_data[k]:\n","      train_img_names.append(i)\n","\n","# Shuffle data and train-validation splitting\n","random.shuffle(train_img_names)\n","\n","# Split in train/validation\n","train_size = int(600 * .7)\n","val_img_names = train_img_names[train_size:].copy()\n","train_img_names = train_img_names[:train_size].copy()\n","\n","# Generate train-validation sets\n","train_dataset = IDDADataset(root = root, list_samples = train_img_names, \n","                            transform = train_transforms, client_name = client_id)\n","\n","val_dataset = IDDADataset(root = root, list_samples = val_img_names, \n","                          transform = val_transforms, client_name = client_id)\n","\n","# Test set\n","with open(os.path.join(root, 'test_same_dom.txt'), 'r') as f:\n","    test_same_dom_data = f.read().splitlines()\n","    test_same_dom_dataset = IDDADataset(root = root, list_samples = test_same_dom_data, \n","                                        transform = test_transforms,\n","                                        client_name='test_same_dom')\n","with open(os.path.join(root, 'test_diff_dom.txt'), 'r') as f:\n","    test_diff_dom_data = f.read().splitlines()\n","    test_diff_dom_dataset = IDDADataset(root=root, list_samples = test_diff_dom_data, \n","                                        transform = test_transforms,\n","                                        client_name='test_diff_dom')\n","\n","# Loss function\n","criterion = torch.nn.CrossEntropyLoss(ignore_index = 255)\n","\n","# Optimizer\n","##################################################################################\n","# Comb_list formats: [<Optimizer>, <learning_rate>, <batch_size>, <transforms>] #\n","#################### [     0     ,        1       ,       2     ,       3     ] #\n","##################################################################################\n","if hyperparameters[comb][0] == 'SGD':\n","  optimizer = torch.optim.SGD(model.parameters(), lr = hyperparameters[comb][1],\n","                              momentum = .9, weight_decay = 1e-4)\n","elif hyperparameters[comb][0] == 'Adam':\n","  optimizer = torch.optim.Adam(model.parameters(), lr = hyperparameters[comb][1])\n","\n","\n","\n","# Data Loaders\n","train_loader = torch.utils.data.DataLoader(train_dataset, \n","                                           batch_size = hyperparameters[comb][2],\n","                                           shuffle=True, num_workers=2)\n","val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = 16,\n","                                           shuffle=True, num_workers=2)\n","test_same_dom_loader = torch.utils.data.DataLoader(test_same_dom_dataset, \n","                                                   batch_size = 16, \n","                                                   shuffle=True, num_workers=2)\n","test_diff_dom_loader = torch.utils.data.DataLoader(test_diff_dom_dataset, \n","                                                   batch_size = 16,\n","                                                   shuffle=True, num_workers=2)\n","\n","# Maximum number of epochs\n","epochs = 20\n","\n","# Training loop\n","# Loss tracking\n","train_loss_list = []\n","train_miou_list = []\n","val_loss_list = []\n","val_miou_list = []\n","test_same_dom_loss_list = []\n","test_same_dom_miou_list = []\n","test_diff_dom_loss_list = []\n","test_diff_dom_miou_list = []\n","\n","# Epochs loop\n","for epoch in range(epochs):\n","\n","  # Training\n","  train_loss, train_miou = train(model, train_loader, criterion,\n","                                     optimizer)\n","  \n","  # Validation\n","  val_loss, val_miou = test(model, val_loader, criterion)\n","\n","  # Evaluation\n","  test_same_dom_loss, test_same_dom_miou = test(model, test_same_dom_loader, criterion)\n","  test_diff_dom_loss, test_diff_dom_miou = test(model, test_diff_dom_loader, criterion)\n","\n","  # Store training and validation losses and accuracies\n","  train_loss_list.append(train_loss)\n","  train_miou_list.append(train_miou)\n","  val_loss_list.append(val_loss)\n","  val_miou_list.append(val_miou)\n","  test_same_dom_loss_list.append(test_same_dom_loss)\n","  test_same_dom_miou_list.append(test_same_dom_miou)\n","  test_diff_dom_loss_list.append(test_diff_dom_loss)\n","  test_diff_dom_miou_list.append(test_diff_dom_miou)\n","\n","  # Display epoch results\n","  # if (epoch + 1) % 100 == 0:\n","  print('-----------------------------------------------------')\n","  print(f'\\tEpoch: {epoch + 1}')\n","  print('\\t Training loss {:.5f}, Training Mean IoU {:.2f}'.format(train_loss,\n","        train_miou['Mean IoU']))\n","  print('\\t Validation loss {:.5f}, Validation Mean IoU {:.2f}'.format(val_loss,\n","        val_miou['Mean IoU']))\n","  print('\\t Test same dom loss {:.5f}, Test same dom Mean IoU {:.2f}'.format(test_same_dom_loss,\n","        test_same_dom_miou['Mean IoU']))\n","  print('\\t Test diff dom loss {:.5f}, Test diff dom Mean IoU {:.2f}'.format(test_diff_dom_loss,\n","        test_diff_dom_miou['Mean IoU']))\n","  print('-----------------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fHgpcINiHBjY"},"outputs":[],"source":["# Import json\n","import json\n","\n","# Define file name\n","filename = 'results'\n","for hyper in hyperparameters[comb]:\n","  filename += '_'\n","  filename += str(hyper) \n","filename += '.json'\n","\n","# Save results on a dict\n","results_dict = {\n","                'Train': (train_loss_list, train_miou_list), \n","                'Validation': (val_loss_list, val_miou_list),\n","                'Test - Same Dom': (test_same_dom_loss_list, \n","                                    test_same_dom_miou_list),\n","                'Test - Diff Dom': (test_diff_dom_loss_list, \n","                                    test_diff_dom_miou_list)\n","              }\n","\n","# Save data on file system (Remember to download it!)\n","with open(filename, 'w') as fp:\n","  json.dump(results_dict, fp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aLNuQM2HUyDe"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"no11Loy0UyFX"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P1qhILNcUyHo"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DnfUYC3EUyJ5"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CHln9k8ZUyNg"},"outputs":[],"source":[]}],"metadata":{"colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
