{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"mFEp1V5RlrcK"},"source":["**IMPORTANTE:** \n","- Il runtime di default è **CPU** per non consumare le ore sulle **GPU**, ricordarsi di cambiarlo;\n","\n","**Collegamento a myDrive per download del dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2339,"status":"ok","timestamp":1685279451022,"user":{"displayName":"Alessandro Licciardi","userId":"05864021108147512422"},"user_tz":-120},"id":"8PN5CgCglhkR","outputId":"4ec69835-cd9c-4a11-e602-23d661d39716"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"NaXuHszwnMGj"},"source":["**Clone del repository github**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":380,"status":"ok","timestamp":1685279451389,"user":{"displayName":"Alessandro Licciardi","userId":"05864021108147512422"},"user_tz":-120},"id":"O8evumfrllb6","outputId":"3fe52a0d-bbf1-4580-b96e-98606eb97df1"},"outputs":[],"source":["!git clone https://github.com/fgiacome/MLDL23-FL-project.git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1685279451390,"user":{"displayName":"Alessandro Licciardi","userId":"05864021108147512422"},"user_tz":-120},"id":"kKNk_DTUlliz","outputId":"3adbc266-8d20-41d2-8502-c04df11654f3"},"outputs":[],"source":["import sys\n","sys.path.append('./MLDL23-FL-project/')\n","%cd MLDL23-FL-project"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"hHFbOs70JDLc"},"source":["**Training and test function**"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1685279451390,"user":{"displayName":"Alessandro Licciardi","userId":"05864021108147512422"},"user_tz":-120},"id":"sgQkQMdHzMPm"},"outputs":[],"source":["# Useful function for model training and testing\n","# Import Intersect Over Union\n","from utils.stream_metrics import StreamSegMetrics\n","from utils.utils import MeanReduction\n","\n","# Training function\n","def train(net, data_loader, loss_function, optimizer):\n","\n","  # Loop initialization\n","  cumulative_loss = 0\n","  samples = 0\n","  mean_iou = StreamSegMetrics(n_classes = 16, name = 'Mean IoU')\n","  reduction = MeanReduction()\n","\n","  # Set the training mode to the model\n","  net.train()\n","\n","  # Loop over batches\n","  for idx, (X, y) in enumerate(data_loader):\n","\n","    # Send data to GPU\n","    X = X.cuda()\n","    # y = y.long()\n","    y = y.cuda()\n","\n","    # Predictions\n","    y_hat = net(X)['out']\n","    y_pred = torch.argmax(y_hat, dim=1)\n","\n","    # Compute loss\n","    criterion = loss_function(y_hat, y)\n","    loss = reduction(criterion, y)\n","\n","    # Backpropagation\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","\n","    # Update training variables\n","    cumulative_loss += loss.item() \n","    samples += X.size(0)\n","    mean_iou.update(y.cpu().numpy(), y_pred.cpu().numpy())\n","\n","  return cumulative_loss / samples, mean_iou.get_results()\n","\n","\n","\n","# Test function\n","def test(net, data_loader, loss_function):\n","\n","  # Test variables initialization\n","  cumulative_loss = 0\n","  samples = 0\n","  mean_iou = StreamSegMetrics(n_classes = 16, name = 'Mean IoU')\n","  reduction = MeanReduction()\n","\n","  # Set the evaluation mode to the model\n","  net.eval()\n","\n","  # Test loop (we don't want to compute the gradients)\n","  with torch.no_grad():\n","\n","    # Loop over batches\n","    for idx, (X, y) in enumerate(data_loader):\n","\n","      # Send data to the GPU\n","      X = X.cuda()\n","      # y = y.long()\n","      y = y.cuda()\n","\n","      # Compute predictions\n","      y_hat = net(X)['out']\n","      y_pred = torch.argmax(y_hat, dim = 1)\n","      \n","      # Loss computation\n","      criterion = loss_function(y_hat, y)\n","      loss = reduction(criterion, y)\n","\n","      # Update test variables\n","      cumulative_loss += loss.item()\n","      samples += X.size(0)\n","      mean_iou.update(y.cpu().numpy(), y_pred.cpu().numpy())\n","\n","  return cumulative_loss / samples, mean_iou.get_results()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"yjJF4V7F0DLG"},"source":["**Hyperparams to test**"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1685279451390,"user":{"displayName":"Alessandro Licciardi","userId":"05864021108147512422"},"user_tz":-120},"id":"Yb6YwVgV0DZU"},"outputs":[],"source":["# Hyperparameters\n","# values: \n","# - beta \\in {1e-4, 1e-3, 1e-2} (see how it affects on paper [12] on FDA)\n","# - lr \\in {1e-3, 1e-2, 1e-1} (We use Adam since we have seen that it provides \n","#                              better results without a optimal fine-tuning)\n","# - transforms \\in {True, False}\n","# || TOTAL COMBINATIONS = 18 ||\n","# Parameters lists\n","beta_list = [2e-3, 4e-3, 6e-2]\n","lr_list = [1e-3, 1e-2, 1e-1]\n","transforms_list = [True, False]\n","\n","# If we have to do FDA, please set FDA = True\n","FDA = False\n","\n","# Folder name \n","foldername = 'CP_lr1e'\n","\n","# Combinations initialization\n","if FDA:\n","  idx = 0\n","  hyperparameters = dict()\n","  for transforms in transforms_list:\n","    for beta in beta_list:\n","      for lr in lr_list:\n","        hyperparameters[f'comb{idx + 1}'] = dict()\n","        hyperparameters[f'comb{idx + 1}']['transforms'] = transforms\n","        hyperparameters[f'comb{idx + 1}']['beta'] = beta\n","        hyperparameters[f'comb{idx + 1}']['lr'] = lr\n","        idx += 1\n","else: \n","  idx = 0\n","  hyperparameters = dict()\n","  for transforms in transforms_list:\n","    for lr in lr_list:\n","      hyperparameters[f'comb{idx + 1}'] = dict()\n","      hyperparameters[f'comb{idx + 1}']['transforms'] = transforms\n","      hyperparameters[f'comb{idx + 1}']['lr'] = lr\n","      idx += 1\n","\n","# Results path\n","results_path = '/content/drive/MyDrive/mldl-project-2b/FDA_P1_Models_Checkpoints/'"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"-OW3ikAntSvO"},"source":["**CONTROLLA LA COMBINAZIONE LE FACCIAMO TUTTE CON TRANSFORMS = TRUE**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1685279451390,"user":{"displayName":"Alessandro Licciardi","userId":"05864021108147512422"},"user_tz":-120},"id":"EUedJ5GnjKRs","outputId":"9950c5f8-bc2e-40d7-944e-995218ae4c77"},"outputs":[],"source":["hyperparameters['comb6']"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"GDm-U4ENtYm8"},"source":["**ALE HO VISTO CHE NON HAI CONTROLLATO, CONTROLLA PER FAVORE**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"bYXlQfOExrGT"},"source":["**NO FDA TESTS**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":627429,"status":"ok","timestamp":1685280078816,"user":{"displayName":"Alessandro Licciardi","userId":"05864021108147512422"},"user_tz":-120},"id":"GRLDUiJYxrRx","outputId":"a85c3876-35ea-4030-c40e-f863241c518d"},"outputs":[],"source":["# Importations\n","import main\n","import torch\n","import torch.nn as  nn\n","import random\n","from utils.utils import MeanReduction\n","from datasets.idda import IDDADataset\n","from datasets.GTA import GTADataset\n","from models import deeplabv3, mobilenetv2\n","################################################################################\n","############################## CHOOSE COMB! ####################################\n","################################################################################\n","comb = 'comb6' #################################################################\n","################################################################################\n","############################## CHOOSE COMB! ####################################\n","################################################################################\n","# Fix random seed\n","random.seed(300890)\n","\n","# args for dataset and model importation\n","class args:\n","  dataset = 'idda'\n","  model = 'deeplabv3_mobilenetv2'\n","\n","# Model\n","model = main.model_init(args).cuda()\n","\n","# Transforms\n","if hyperparameters[comb]['transforms'] == True:\n","  gta_transforms, idda_transforms = main.get_transforms(args)\n","else: \n","  _, gta_transforms = main.get_transforms(args)\n","  _, idda_transforms = main.get_transforms(args)\n","\n","# Importations\n","import os\n","import json\n","\n","# Initialization of train IDDA dataset (centralized)\n","root = 'data/idda'\n","client_id = \"CENTRALIZED\"\n","with open(os.path.join(root, 'train.json'), 'r') as f:\n","    all_data = json.load(f)\n","idda_img_names = [] \n","for k in all_data.keys():\n","    for i in all_data[k]:\n","      idda_img_names.append(i)\n","\n","# Generate train-validation sets\n","idda_df = IDDADataset(root = root, list_samples = idda_img_names, \n","                            transform = idda_transforms, client_name = client_id)\n","  \n","# Initialization of test IDDA dataset\n","with open(os.path.join(root, 'test_same_dom.txt'), 'r') as f:\n","    test_same_dom_data = f.read().splitlines()\n","    test_same_dom_df = IDDADataset(root = root, list_samples = test_same_dom_data, \n","                                        transform = idda_transforms,\n","                                        client_name='test_same_dom')\n","\n","with open(os.path.join(root, 'test_diff_dom.txt'), 'r') as f:\n","    test_diff_dom_data = f.read().splitlines()\n","    test_diff_dom_df = IDDADataset(root = root, list_samples = test_diff_dom_data, \n","                                        transform = idda_transforms,\n","                                        client_name='test_diff_dom')\n","\n","# GTA5 directory\n","root = '/content/drive/MyDrive/MLDL_Datasets/GTA5/'\n","filename = 'train.txt'\n","full_path = root + filename\n","\n","# Import GTA5 images\n","lines_clean = []\n","with open(full_path, 'r') as fp:\n","  lines = fp.readlines()\n","for i, line in enumerate(lines):\n","  if line[-1] == '\\n': \n","    lines[i] = line[:-1]\n","  if os.path.exists(root + '/images/' + lines[i]):\n","    lines_clean.append(lines[i])\n","\n","# GTA5 initialization\n","gta_df = GTADataset(root = root, list_samples = lines_clean,\n","                    transform = gta_transforms, client_name='GTA5')\n","\n","# Data Loaders\n","gta_loader = torch.utils.data.DataLoader(gta_df, \n","                                         batch_size = 8,\n","                                         shuffle = True, num_workers = 2)\n","idda_loader = torch.utils.data.DataLoader(idda_df, batch_size = 16,\n","                                          shuffle = True, num_workers = 2)\n","test_same_dom_loader = torch.utils.data.DataLoader(test_same_dom_df, \n","                                                   batch_size = 16, \n","                                                   shuffle = True, num_workers=2)\n","test_diff_dom_loader = torch.utils.data.DataLoader(test_diff_dom_df, \n","                                                   batch_size = 16,\n","                                                   shuffle=True, num_workers=2)\n","\n","# Criterion\n","criterion = nn.CrossEntropyLoss(ignore_index = 255, reduction = 'none')\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), \n","                             lr = hyperparameters[comb]['lr'])\n","\n","# Number of epochs and checkpoint\n","num_epochs = 20\n","checkpoint = 5 # The model is saved every 5 epochs \n","\n","# Initializaion of loss and miou lists for each dataset\n","gta_loss_list = []\n","idda_loss_list = []\n","test_same_dom_loss_list = []\n","test_diff_dom_loss_list = []\n","gta_miou_list = []\n","idda_miou_list = []\n","test_same_dom_miou_list = []\n","test_diff_dom_miou_list = []\n","\n","# Results path\n","foldername += str(len(str(hyperparameters[comb]['lr'])) - 2) + '_' \n","foldername += str(hyperparameters[comb]['transforms'])[0]\n","\n","# Loop on epochs\n","for epoch in range(num_epochs):\n","\n","  # Train the model\n","  gta_loss, gta_miou = train(model, gta_loader, criterion, optimizer)\n","\n","  # Append results\n","  gta_loss_list.append(gta_loss)\n","  gta_miou_list.append(gta_miou)\n","\n","  # Model evaluation\n","  if (epoch + 1) % checkpoint == 0:\n","\n","    checkpoint_path = results_path + foldername + f'/checkpoint_e{epoch + 1}.json'\n","    \n","    # IDDA dataset\n","    idda_loss, idda_miou = test(model, idda_loader, criterion)\n","\n","    # Test datasets\n","    test_same_dom_loss, test_same_dom_miou = test(model, test_same_dom_loader, criterion)\n","    test_diff_dom_loss, test_diff_dom_miou = test(model, test_diff_dom_loader, criterion)\n","\n","    # Append results \n","    idda_loss_list.append(idda_loss)\n","    idda_miou_list.append(idda_miou)\n","    test_same_dom_loss_list.append(test_same_dom_loss)\n","    test_same_dom_miou_list.append(test_same_dom_miou)    \n","    test_diff_dom_loss_list.append(test_diff_dom_loss)\n","    test_diff_dom_miou_list.append(test_diff_dom_miou)    \n","\n","    # Save model \n","    torch.save({\n","                'epoch': epoch,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'results': {\n","                      'GTA': {\n","                          'Loss': gta_loss,\n","                          'Mean IoU': gta_miou\n","                      },\n","                      'IDDA': {\n","                          'Loss': idda_loss,\n","                          'Mean IoU': idda_miou\n","                      },\n","                      'Test Same Dom': {\n","                          'Loss': test_same_dom_loss,\n","                          'Mean IoU': test_same_dom_miou\n","                      },\n","                      'Test Diff Dom': {\n","                          'Loss': test_diff_dom_loss,\n","                          'Mean IoU': test_diff_dom_miou\n","                      }\n","                  },\n","                }, checkpoint_path)\n","    \n","    print('-----------------------------------------------------')\n","    print(f'\\tEpoch: {epoch + 1}')\n","    print('\\t GTA loss {:.5f}, GTA Mean IoU {:.2f}'.format(gta_loss,\n","          gta_miou['Mean IoU']))\n","    print('\\t IDDA loss {:.5f}, IDDA Mean IoU {:.2f}'.format(idda_loss,\n","          idda_miou['Mean IoU']))\n","    print('\\t Test same dom loss {:.5f}, Test same dom Mean IoU {:.2f}'.format(test_same_dom_loss,\n","          test_same_dom_miou['Mean IoU']))\n","    print('\\t Test diff dom loss {:.5f}, Test diff dom Mean IoU {:.2f}'.format(test_diff_dom_loss,\n","          test_diff_dom_miou['Mean IoU']))\n","    print('-----------------------------------------------------')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"z4wEO1Opws_p"},"source":["**Save results**"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1685280078817,"user":{"displayName":"Alessandro Licciardi","userId":"05864021108147512422"},"user_tz":-120},"id":"fHgpcINiHBjY"},"outputs":[],"source":["# Import json\n","import json\n","\n","# Path\n","results_path = '/content/drive/MyDrive/mldl-project-2b/FDA_P1_Models_Checkpoints/Results_default/'\n","results_path += foldername\n","results_path += '.json'\n","\n","# Save results on a dict\n","results_dict = {\n","                'GTA': (gta_loss_list, gta_miou_list), \n","                'IDDA': (idda_loss_list, idda_miou_list),\n","                'Test Same Dom': (test_same_dom_loss_list, \n","                                    test_same_dom_miou_list),\n","                'Test Diff Dom': (test_diff_dom_loss_list, \n","                                    test_diff_dom_miou_list)\n","              }\n","\n","# Save data on file system (Remember to download it!)\n","with open(results_path, 'w') as fp:  \n","  json.dump(results_dict, fp)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"l7uwdEvcWQ1K"},"source":["- RandomFDA, inizializzare solo con beta;\n","- servono add_style e get_style_from_img \n","\n","chiamata statica\n","RandomFourierDomainAdaptation.get_style_from_img,\n","\n","- l'immagine da passare deve essere PIL (su importazione)\n","- l'immagine da chiamare è dal target dataset (adatto gta sul client)\n","devo ciclare idda dataset e per tutte le immagine devo chiamare get_style_from_img (apri manualmente con open).\n","\n","- salvare np.ndarray in una variabile provvisoria"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
